\chapter{Related Works}

Jester is similar to several existing frameworks for abstracting different parts of the skeletal data input process. Most of the research in this area to date has been focused on either a lower or higher level of abstraction and does not provide the flexibility that is built into Jester

\section{OpenNI}

OpenNI is a nonprofit organization focused on abstracting the process of retrieving depth, standard RGB, or audio data from sensors. Their API is designed to work with third party middleware in order to provide analysis and processing functionality to a wide range of sensors with minimal overhead. The project is open source and allows developers to write wrappers for their sensor drivers to port data into OpenNI’s input framework. The OpenNI API allows developers to have a single point of access for operations like detecting if devices are connected to the system or starting and stopping data streams. Providing developers with a single interface for communicating with sensors greatly simplifies the development process because the problems associated with supporting and compiling against multiple APIs are eliminated. It also benefits hardware manufacturers by making it easier for developers to adopt new sensors.

A majority of the hardware that are supported by the OpenNI API is manufactured by the companies that formed the partnership that founded OpenNI. There are some community written sensor wrappers, but they can be difficult to locate because there is no central repository. PrimeSense, the major driving force behind the OpenNI project, was purchased by Apple in November of 2013 and is no longer providing active support. The main web portal for downloading OpenNI and middleware binaries has been taken down. There is a GitHub repository for the OpenNI source, but it is no longer maintained.

\section{The Point Cloud Library}

The Point Cloud Library (PCL) is a large scale open source project for processing point cloud data. Point clouds are a method for representing range data where each each range measurement is stored as a 3D point in space. These points resemble clouds when visualized in 3D. The PCL has extensive support for data filtering, feature detection, registering clouds between frames to determine movement, and surfacing. It also has input modules that accept a large variety of depth sensors. Essentially any sensor that outputs 3D depth information can be adapted to work with the PCL.

The PCL is used heavily by industry and research and is integrated with many open source and proprietary projects. It is an excellent tool for creating software that can manipulate the raw depth data from sensors and would be a good choice for writing an API that can estimate skeletal position using any depth sensor. The PCL’s abstraction of sensor input using point clouds in order to build a sophisticated higher level feature set is precisely what Jester is trying to achieve for taking bone position data and providing a framework to easily fuse and filter data and then represent the skeleton in a consistent model for easy manipulation.

\section{Vrui Toolkit}

The Vrui Toolkit is an open source development toolkit that allows VR application developers to be shielded from the input and output configuration details of a virtual environment. Vrui supports display abstraction by creating OpenGL contexts that can be assigned to any combination of monitors or head mounted displays as well as a library of 3D UI widgets that allow developers to create 3D interfaces in the same manner as traditional 2D GUI editors. Input is abstracted with “semantic events” that notify the application about interaction events and input from a set of input tools like 3D pointers or 2D mice. It also provides a framework for splitting the input and output abstractions onto remote systems to allow parallelism and remote operation. Vrui is designed to provide a full suite of common VR functionality with dependencies that have excellent cross operating system support in order to maximize system compatibility. 

Vrui attempts to tackle the VR abstraction problem with a monolithic software architecture that can handle the entire stack from hardware data capture to output. This massive software stack makes it difficult to change the functionality of any sub unit without side effects. It is also difficult to use the Vrui input abstraction without the rest of the system which arbitrarily increases the complexity of attempting to process the sensor driven input events in a different manner than the intended design. These constraints are not necessarily a problem for 3D applications that follow more of a 2D input paradigm, but it does limit flexibility. Vrui also does not have any support for fusing input data from separate sources.

\section{MiddleVR}

MiddleVR is closed source middleware that is designed to simplify the creation and deployment of VR applications. It is designed, like Vrui, to handle all aspects of building a VR application. It has an extensive supported input and output devices list that allows developers to seamlessly use a wide variety of devices. MiddleVR can use the Unity framework for output. Unity is an existing framework that allows applications to be easily ported between operating systems and even hardware architectures. The company that produces MiddleVR, “i’m in VR”, sells it as a product, so using it for commercial purposes or without watermarks is not free.

Since MiddleVR is closed source, it is not possible for the developer community to add support for more devices without the cooperation of “i’m in VR”. This slows the adoption of new hardware and makes it less appealing. It also does not expose the skeletal structure of the user and instead relies heavily on the idea of extending the concepts 2D pointing devices into 3D without treating the human body as a controller by itself. MiddleVR also lacks any sort of data fusion framework.

\section{Virtual Reality Peripheral Network}

The Virtual Reality Peripheral Network (VRPN) is an open source project that was released in 1998. It provides a device agnostic and network transparent server-client architecture for accessing virtual reality devices. VRPN uses device server nodes to wrap sensor specific functions into a common communication protocol and VR client nodes to listen to broadcast sensor data updates. It is free and supports multiple operating system platforms.

VRPN currently only supports the kind of devices that extend the 2D pointing paradigm to 3D. Since it was created so long ago, it is not well suited to the skeletal model that is popular in current 3D input systems. While network transparency is a cool feature, it is not necessary in the style of systems that are currently used in the consumer sector. Like all of the other examples, there is no support for data fusion from multiple sensing sources since fusing multiple 3D pointers is not meaningful.