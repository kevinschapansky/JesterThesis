\chapter{Introduction}

Since the introduction of personal computers with graphical user interfaces, hardware mice have been the dominant way that humans interact with the two dimensional world. Computer mice are very simple to manufacture and are an effective tool for interacting with strictly two dimensional applications such as word processing or managing an email inbox because they are bound to two dimensional sensing surfaces like a table top or flat laptop trackpad. Traditional computer programs and operating systems are designed to follow the "Windows, Icons Menus, Pointer" (WIMP) computer interaction paradigm. WIMP attempts to make a good digital analogue for a physical desktop where documents and folders are essentially managed in two dimensions \cite{hinckley2002input}.

Even though using a mouse for WIMP-style two dimensional applications is a fairly simple task that even small children master quickly, it quickly becomes a handicap when trying to manipulate data that is inherently 3D or can be easily mapped to 3D; eg. medical imaging and mechanical CAD or file management \cite{krueger1991artificial}. Since humans live in a 3D world by default, the human body itself makes an excellent 3D input device, just like how a computer mouse bound to sensing a two dimensional table top makes an excellent two dimensional sensing device \cite{wren1996pfinder}.

\section{Gesture HCI}

Humans live every day of their lives in a 3D world. Our bodies are 3D and every physical object with which we interact is 3D. This makes the human body, and generally the hands, a natural choice for interacting with computers in a more intuitive manner. For example, when a user first attempts to interact with 3D Computer Aided Design (CAD) software with a mouse, there is a steep learning curve before they can fluidly and correctly navigate through the world. Different mouse or keyboard buttons are generally used to differentiate between scaling, rotating, and translating the object that is being designed \cite{hamade2007evaluating}. Application specific key combinations are not innately intuitive, even if they do become second nature after extended use of the software.The user's hands are a perfect input choice because they are the interface they have been using to interact with their world ever since they were able to correctly place a triangle shaped block in a triangle shaped hole. Tracking the user's hand and finger position allows object manipulation in CAD software to become innately intuitive through the use of standard grabbing, rotating, and pushing or pulling gestures \cite{nam1996recognition}.

Nintendo introduced the concept of games that are controlled by the human body to millions of users in 2006. The Nintendo Wii includes sensors that can detect the position of user's hands and comes bundled with a simulated sports package that allows users to use their arms to intuitively play digital sports \cite{pasch2009movement}. In 2011, Microsoft brought marker-less gesture interaction to mainstream gaming with the introduction of the Kinect for Xbox \cite{kinectlaunch}. The Kinect uses a technology called structured light, which will be discussed in the next chapter, to track the position of a player's entire body. Human body tracking allows the users to control the Xbox using arm movements without picking up a controller or even play a game that allows users to compete against each other in a virtual dance off \cite{zhang2012microsoft}.

Gesture interaction has also been applied to tasks that do not have as clear of a mapping to real life as CAD or video games. For example, gestures can be used to easily switch between browser tabs or navigate web pages \cite{moyle2003design}. Also, work is being done on creating 3D file browsers where users can “shuffle through” digital documents and spreadsheets just like they would with physical paper. The task of arranging and moving files between folders can also be intuitively modeled using grab, move, and drop gestures that simulate arranging physical objects \cite{dnd_interface}.

\section{The Human Skeleton}

In order to accurately track the position of a human's body it is necessary to pick some form of conceptual model that can accurately represent it. Muscle and connective tissue systems are extremely complex, and would be very difficult to represent intuitively or with any reasonable accuracy. The human skeleton is an excellent choice for representing body position since it is the frame to which every other part of the body attaches. It is not necessary to track all 206 bones in the body since many bones can be approximated as a single bone, such as the bones in the feet or spinal column, or are simply not necessary to establish the position of a person; e.g. the ribs. A simplified skeletal rigging system is frequently used in computer animation to recreate how muscle and skin move in response to changing skeletal position \cite{parent2012computer}. The reduced skeleton is both conceptually simple and can be used to accurately represent body position. Most 3D input devices have associated APIs that provide data about either skeletal joints, which can be trivially used to infer bone position, or direct bone position.

\section{3D Input Devices}

Historically, 3D input systems have been cumbersome and very expensive. They required complex sensor systems that either mounted to the body, or relied on the user wearing a skinsuit covered in motion markers \cite{moeslund2006survey}. There are also early versions that use computer vision techniques, but they were notoriously inaccurate and sensitive to lighting and user clothing \cite{moeslund2006survey}. The expense of setting up a sensor room or the hassle of wearable instrumentation essentially guaranteed that these systems were only used in academic research or high budget special effects.

Within the last two years, there has been an explosion of fairly accurate and consumer-affordable 3D input devices. In 2011 Microsoft unveiled the Kinect for the Xbox 360 \cite{kinectlaunch}. The Kinect is capable of fairly accurately tracking the user's torso and limbs, but not finer details like fingers. Several other devices like the Leap Motion Controller, released in 2013,  and the Creative Senz3D, released in 2014, are capable of accurately tracking hands and fingers \cite{weichert2013analysis,kratz2013depth}. Finger tracking sensors generally have a limited field of view and range so they cannot track a user’s entire body. There are also hardware-based sensors like the Razer Hydra or the Oculus Rift that are capable of tracking with very high precision and minimal setup time \cite{basu2012immersive}. Each of these sensors has its own unique API for querying skeletal data.

\section{3D Applications}

Many of the initial applications designed to take advantage of 3D input devices were games and targeted to be more entertaining than useful. There are several games for Xbox that use the Kinect to judge a user’s dancing skill \cite{kinect_dance}. Several PC games have also been ported to use the Oculus Rift and Razer Hydra to provide a more immersive gaming experience \cite{rift_hydra}. A majority of the applications that support the Leap Motion Controller are games, but some scientific visualization and CAD applications have introduced Leap support. There has been work on using the Leap Motion and other short range finger sensing devices to navigate desktop workspaces or web browsing, but it is mostly experimental \cite{moyle2003design}. All of these applications only support a small subset of the sensors that could be used to accomplish the same level of control.

\section{Jester's Contribution}

The human body is a proven method for intuitively and accurately interacting with simulated 3D environments and new and affordable 3D input devices targeted at consumers are flooding the market. However, there is still a significant barrier preventing 3D applications from taking off. Developers must individually tailor their applications to support specific sensors. Also, since sensors that are good at tracking details can rarely see a user's whole body and wide angle sensors cannot see enough detail to accurately track fingers, developers must custom craft a data fusion system if they want the best features of both sensors. Jester provides an easily extensible library that simplifies the process of creating an application that can use 3D input devices. It implements a framework for creating simple API translation classes, known as thin-wrappers, to map the skeletal data of any sensor into Jester’s internal model of the skeleton, as well as a framework for filtering and fusing data from multiple sensors. Application designers only need to write a thin wrapper for their sensor of choice that maps positional data to any bone or joint that is being tracked. Their application then has access to a world space skeleton and can be effortlessly adapted to use any available 3D input device.